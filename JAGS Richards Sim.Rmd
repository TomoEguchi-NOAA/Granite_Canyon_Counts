---
title: "R Notebook"
output: html_notebook
---

This notebook describes an attempt to use Richards' function to estimate the true N in the N-mixture model of Granite Canyon counts, when data were simulated from Richards' function. The observed counts are binomial deviates with the "true" N and capture probabilities that are affected by covariates; Beaufort sea state, visibility, and observers.  



Set up libraries and bring in the real data. 

```{r}
rm(list=ls())
library(abind)
library(jagsUI)
library(tidyverse)
library(bayesplot)
# library(mgcv)
# library(splines)

models <- "pois_bino"
#jags.model <- paste0("models/model_Richards_", models, ".txt")
jags.model <- paste0("models/model_Richards_", models, ".txt")

out.file.name <- paste0("RData/JAGS_Richards_v1_", models, 
                        "_sim_", Sys.Date(), ".RData")

# Observer list gets updated as new observers are added. 
# obs.list <- read.csv("Data/Observer list.csv", header = T) 
# colnames(obs.list) <- c("obs", "ID")
# seasons <- c("2006/2007", "2007/2008", "2009/2010", "2010/2011", 
#              "2014/2015", "2015/2016", "2019/2020", "2021/2022")

seasons <- c("2014/2015", "2015/2016", "2019/2020", "2021/2022")

n.seasons <- length(seasons)

# Load the most recent data that were newly extracted for the last four surveys:
# Using Final_Data, which contains only data for observation periods that are 90 +/- 5 minutes
# This can be changed to use all data (Data_Out) in the future 2022-09-06

Final_Data <- list()
k <- 3
for (k in 1:length(seasons)){
  tmp <- readRDS(paste0("RData/V2.1_Aug2022/out_",
                        strsplit(seasons[k], "/")[[1]][2],
                        "_Tomo_v2.rds"))

  #Final_Data[[k]] <- tmp$Final_Data %>%
  Final_Data[[k]] <- tmp$Data_Out %>%
    mutate(Year = as.numeric(strsplit(seasons[k], "/")[[1]][2]),
           Day = as.numeric(BeginDay)) %>%
    # right_join(., data.frame(Day = 1:90,
    #                          n = NA,
    #                          Year = as.numeric(strsplit(seasons[k], "/")[[1]][2])), 
    #            by = "Day") %>%
    # 
    # select(Year.y, Day, n.x, n.y, dur, bf, vs, obs, begin) %>%
    # mutate(n = n.x,
    #        effort = ifelse(is.na(dur), 0, dur),
    #        Year = Year.y) %>%
    # select(-c(Year.y, n.x, n.y, dur)) %>%
    # arrange(Day) 
    mutate(effort = dur) %>%
    select(Year, Day, effort, bf, vs, n, obs)
  
    #complete(Day, i) %>%
    #filter(!is.na(i))

}

Final_Data.all <- do.call(rbind, Final_Data)
Final_Data.all$St <- 1            # for this one, we only have 1 station (2022-08-19)

# Find all observers and change them into integer code:
unique.obs <- unique(Final_Data.all$obs)
unique.obs <- unique.obs[!is.na(unique.obs)]
obs.ID.df <- data.frame(obs = unique.obs,
                        obs.ID = 1:length(unique.obs))

Final_Data.all %>% 
  left_join(obs.ID.df, by = "obs") %>% 
  select(-obs) -> Final_Data.all

# figure out the number of periods
Final_Data.all %>% 
  group_by(Year) %>%
  #filter(effort > 0) %>%
  summarise(n = n()) -> periods

Final_Data.all$n[Final_Data.all$effort == 0] <- NA

years <- unique(Final_Data.all$Year)


```

The daily mean is a function of days (1:90)

```{r data_setup}
Final_Data.all %>% 
  group_by(Year, Day) %>%
  summarise(daily.effort = sum(effort, na.rm = T)) %>%
  pivot_wider(values_from = daily.effort,
              names_from = Year,
              names_prefix = "Y")-> daily.effort

# daily.effort %>%
#   #group_by(Day) %>%
#   summarise_at(vars(Y2015:Y2022), funs(Day[. > 0][1])) %>%
#   select(starts_with("Y")) -> first.day

Final_Data.all %>% 
  group_by(Year, Day) %>%
  summarise(daily.counts = sum(n, na.rm = T)) %>%
  pivot_wider(values_from = daily.counts,
              names_from = Year,
              names_prefix = "Y")-> daily.counts

Final_Data.all %>%
  group_by(Year, Day) %>%
  summarise(daily.effort = sum(effort, na.rm = T),
            daily.counts = sum(n, na.rm = T)) %>%
#  mutate(daily.N = ceiling(daily.counts/daily.effort) ) %>%
  mutate(daily.N = daily.counts ) %>%
  select(-c(daily.effort, daily.counts)) %>%
  pivot_wider(values_from = daily.N,
              names_from = Year,
              names_prefix = "Y") %>%
  right_join(y = data.frame(Day = seq(1:90)),
             by = "Day") %>%
  arrange(Day) -> daily.N


daily.N[is.na(daily.N)] <- 0

Final_Data.all %>% 
  group_by(Year, Day) %>%
  summarise(daily.obs = first(obs.ID)) %>%
  pivot_wider(values_from = daily.obs,
              names_from = Year,
              names_prefix = "Y")-> daily.obs

n.per.day <- obs.all <- n.all <- array(data = 0, 
                                       dim = c(max(periods$n), 2, length(seasons)))

days.all <- bf.all <- vs.all <- watch.prop <- matrix(data = NA,
                                                     nrow = max(periods$n), 
                                                     ncol = length(seasons))

for (k in 1:length(seasons)){
  n.all[1:periods$n[k],1, k] <- Final_Data[[k]]$n
  n.per.day[1:periods$n[k],1, k] <- Final_Data[[k]]$n/Final_Data[[k]]$effort
  bf.all[1:periods$n[k],k] <- Final_Data[[k]]$bf
  vs.all[1:periods$n[k],k] <- Final_Data[[k]]$vs
  # 540 minutes (maximum observation duration in a day) in unit of days
  watch.prop[1:periods$n[k],k] <- Final_Data[[k]]$effort/(540/24/60)
  
  days.all[1:periods$n[k],k] <- Final_Data[[k]]$Day
  
  obs.all[1:periods$n[k],1, k] <- Final_Data.all %>% 
    filter(Year == years[k]) %>%
    select(obs.ID) %>% 
    pull() 
}

#bf.all[is.na(bf.all)] <- 0
#vs.all[is.na(vs.all)] <- 0
obs.all[is.na(obs.all)] <- max(obs.ID.df$obs.ID) + 1
n.per.day[is.na(n.per.day)] <- 0

```



Simulate data:

```{r}
set.seed(12345)

# simulate counts from a known distribution:
obs <- array(rdunif(n = prod(dim(obs.all)), 
                    a = 1, b = max(obs.all)+1),
             dim = dim(obs.all))
             
# bf <- matrix(rdunif(n = prod(dim(bf.all)), a = 0, b = 4),
#              nrow = nrow(bf.all), ncol = ncol(bf.all))
# 
# vs <- matrix(rdunif(n = prod(dim(bf.all)), a = 0, b = 4),
#              nrow = nrow(bf.all), ncol = ncol(bf.all))

#logit <- function(p) log(p/(1-p))

prob <- lm. <- array(data = 0, dim = dim(obs))

BF.fixed <- -4
VS.fixed <- -2
mean <- 0.3
for (c in 1:dim(prob)[3]){
  for (r in 1:dim(prob)[1]){
    for (s in 1:2){
      lm.[r,s,c] <- mean + 
        BF.fixed * bf.all[r,c] + 
        VS.fixed * vs.all[r,c] + 
        obs[r,s,c] + 
        rnorm(n = 1, mean = 0, sd = 0.1) 
      
      prob[r,s,c] <- 1/(1 + exp(-lm.[r,s,c])) * watch.prop[r,c]
    }
    
  }
}

Richards_fcn <- function(d, S1, S2, K, P, min, max){
  K <- abs(K)
  # S <- abs(S)
  # S1 <- -S
  M1 <- (1 + (2 * exp(K) - 1) * exp((1/S1) * (P - d))) ^ (-1/exp(K))
  M2 <- (1 + (2 * exp(K) - 1) * exp((1/S2) * (P - d))) ^ (-1/exp(K))
  N <- min + (max - min) * (M1 * M2)
  return(N)
}
S1 <- -1 * c(0.5, 1.2, 1.4, 0.9)
S2 <- c(1.5, 2.1, 1.2, 1.9)   # fatness
K <- c(1.0, 1.2, 1.3, 1.2)   # peak flatness
P <- c(40, 45, 50, 42)       # peak timing
max.N <- c(800, 850, 820, 600)
CV <- 0.3

true.mean.N <- N <- matrix(data = 0, nrow = 90, ncol = dim(prob)[3])

for (t in 1:dim(prob)[3]){
  for (d in 1:90){
    true.mean.N[d,t] <- floor(Richards_fcn(d = d, 
                                      S1 = S1[t],
                                      S2 = S2[t],
                                      K = K[t], 
                                      P = P[t], 
                                      min = 0, max = max.N[t])  )
    
    N[d,t] <- floor(rnorm(n = 1, 
                          mean = true.mean.N[d,t],
                          sd = CV * true.mean.N[d,t]))
    
  }
}

obsd.n <- array(data = 0, dim = dim(prob))

for (t in 1:dim(prob)[3]){
  for (s in 1:2){
    for (k in 1:dim(prob)[1]){
      obsd.n[k,s,t] <- ifelse(is.na(prob[k,s,t]),
                              NA,       
                              rbinom(n = 1, 
                                     size = N[days.all[k,t],t], 
                                     prob = prob[k,s,t]))
      
    }
    
  }
}


```

$$M_1 = (1 + (2 e^K - 1) * e^{(P-d)/(-S)}) ^ {(-1/e^K)}$$

$$M_2 = (1 + (2 e^K - 1) * e^{(P-d)/(S)}) ^ {(-1/e^K)}$$

$$N = min_N + (max_N - min_N) * (M_1 * M_2),$$ 

where $d$ is the number of days from the beginning of nesting season,

$S$ defines the "fatness" of the function ($S > 0$),

$K > 0$ defines the "flatness" at the peak of the function,

$P$ defines where the peak is relative to the range of $d$, where $min(d) < P < max(d)$,

$min_N$ is "the basal level of nesting outside the nesting season" and,

$max_N >> min_N$



```{r run-jags}
MCMC.params <- list(n.samples = 100000,
                    n.thin = 10,
                    n.burnin = 80000,
                    n.chains = 5)

jags.params <- c("lambda","OBS.RF","OBS.Switch",
                "BF.Switch","BF.Fixed","VS.Switch",
                "VS.Fixed","mean.prob",
                "BF.Fixed",
                "VS.Fixed", "mean.N", "max",
                "Corrected.Est","Raw.Est","N",
                "K", "S1", "S2", "P", "cv.N",
                "sigma.N", "log.lkhd")

jags.data <- list(  n = obsd.n, 
                    n.station = c(1,1,1,1),
                    n.year = length(seasons),
                    n.obs = nrow(obs.ID.df)+1,
                    periods = periods$n,
                    #Daily.N = as.matrix(daily.N[,2:(length(seasons)+1)]),
                    #first.day = unlist(as.vector(first.day)),
                    obs = obs.all,
                    vs = vs.all,
                    bf = bf.all,
                    watch.prop = watch.prop,
                    day = days.all,
                    max.vec = rep(1000, times = 4))

jm <- jags(jags.data,
             inits = NULL,
             parameters.to.save= jags.params,
             model.file = jags.model,
             n.chains = MCMC.params$n.chains,
             n.burnin = MCMC.params$n.burnin,
             n.thin = MCMC.params$n.thin,
             n.iter = MCMC.params$n.samples,
             DIC = T, parallel=T)

```


## Check results {-}

### K parameter (Flatness of the peak)

Year-specific K do not converge well, at least for the 4-year data set. 

```{r}
# mcmc_trace(jm$samples, c("K[1]", "K[2]",
#                          "K[3]", "K[4]"))

mcmc_trace(jm$samples, "K")

```



```{r}
# mcmc_dens(jm$samples, c("K[1]", "K[2]",
#                          "K[3]", "K[4]"))

mcmc_dens(jm$samples, "K")

```


### S1 parameter (Declining slope)

```{r}
mcmc_trace(jm$samples, c("S1[1]", "S1[2]",
                         "S1[3]", "S1[4]"))

```




```{r}
mcmc_dens(jm$samples, c("S1[1]", "S1[2]",
                         "S1[3]", "S1[4]"))

```


### S2 parameter (Increasing slope)

```{r}
mcmc_trace(jm$samples, c("S2[1]", "S2[2]",
                         "S2[3]", "S2[4]"))

```




```{r}
mcmc_dens(jm$samples, c("S2[1]", "S2[2]",
                         "S2[3]", "S2[4]"))

```


### P parameter (Peak)

```{r}
mcmc_trace(jm$samples, c("P[1]", "P[2]",
                         "P[3]", "P[4]"))


```



```{r}
mcmc_dens(jm$samples, c("P[1]", "P[2]",
                         "P[3]", "P[4]"))

#mcmc_trace(jm$samples, c("K", "S"))

```


### Fixed parameters for sighting probabilities (Beaufort (BF) and visibility (VS))

```{r}
mcmc_trace(jm$samples, c("BF.Fixed", "VS.Fixed"))
```


### max parameter 

# They look pretty much identical... may be we don't need year-specific. 

```{r}
# mcmc_trace(jm$samples, c("max[1]", "max[2]",
#                          "max[3]", "max[4]"))

mcmc_trace(jm$samples, "max")

```



```{r}
#mcmc_dens(jm$samples, c("max[1]", "max[2]",
#                         "max[3]", "max[4]"))

mcmc_dens(jm$samples, c("max"))

```

```{r}
#mcmc_trace(jm$samples, "cv.N")
```


### Richards function 

True function is in gold. 


```{r}

mean.N.hats <- data.frame(Season = rep(seasons, each = 90),
                          Day = rep(1:90, times = 4),
                          Mean = as.vector(jm$mean$mean.N),
                          LCL = as.vector(jm$q2.5$mean.N),
                          UCL = as.vector(jm$q97.5$mean.N),
                          true.mean = as.vector(true.mean.N))

                     
ggplot(mean.N.hats %>% group_by(Season)) + 
  geom_ribbon(aes(x = Day, ymin = LCL, ymax = UCL),
              fill = "blue", alpha = 0.5) +
  geom_path(aes(x = Day, y = Mean)) + 
  geom_path(aes(x = Day, y = true.mean), color = "gold") +
  facet_wrap(~ Season)



```


### Accuracy of N estimates (true values are in gold)

```{r}

N.hats <- data.frame(Season = rep(seasons, each = 90),
                     Day = rep(1:90, times = 4),
                     Mean = as.vector(jm$mean$N),
                     LCL = as.vector(jm$q2.5$N),
                     UCL = as.vector(jm$q97.5$N),
                     true.N = as.vector(N))


ggplot(N.hats %>% group_by(Season)) + 
  geom_ribbon(aes(x = Day, ymin = LCL, ymax = UCL),
              fill = "blue", alpha = 0.5) +
  geom_path(aes(x = Day, y = Mean)) + 
  geom_path(aes(x = Day, y = N), color = "gold") +
  facet_wrap(~ Season)



```


### Posteriors on corrected estimates 

```{r}
# Corrected Estimates - the abundance
mcmc_trace(jm$samples, c("Corrected.Est[1]", "Corrected.Est[2]",
                         "Corrected.Est[3]", "Corrected.Est[4]"))


# This is the output file name
# jags.out.file <- paste0("RData/jagam_dailyN_k", 
#                         basis.dim, "_", family, "_jags.rds")

```


```{r}
mcmc_dens(jm$samples, c("Corrected.Est[1]", "Corrected.Est[2]",
                         "Corrected.Est[3]", "Corrected.Est[4]"))

```

Data

```{r}
n.st1 <- obsd.n[,1,]
obsd.n <- data.frame(Season = rep(seasons, each = nrow(n.st1)),
                     Day = as.vector(days.all),
                     n = as.vector(n.st1)) %>% na.omit()
                     
ggplot(obsd.n %>% group_by(Season)) +
  geom_point(aes(x = Day, y = n)) +
  facet_wrap(~Season)
```

Save results

```{r}

#save.image(file = "RData/Richards_norm_bino_sim_2022-09-09.RData")
save.image(file = out.file.name)

```

