---
title: "R Notebook"
output: html_notebook
---

This notebook includes the part that a polynomial regression is used to model "true" N as a function of the day of the season. The  observed counts are binomial deviates with the "true" N and capture probabilities that are affected by covariates; Beaufort sea state, visibility, and observers.  


The station information needs to be added. The data files that I have don't have that info... this needs to be fixed in the future. 

Set up libraries

```{r}
rm(list=ls())
library(abind)
library(jagsUI)
library(tidyverse)
library(bayesplot)
library(mgcv)
library(splines)

jags.model <- paste0("models/Model_dailyN_spline_Nmix_bin_negbin_JAGS.txt")

# from https://github.com/andrewcparnell/jags_examples/blob/master/R%20Code/jags_spline.R
# A function that uses the bs() function to generate the B-spline basis functions
# following Eilers and Marx 'Craft of smoothing' course. This bs_bbase() function
# is equivalent to the bbase() function available at http://statweb.lsu.edu/faculty/marx/

bs_bbase <- function(x, xl = min(x), xr = max(x), nseg = 10, deg = 3) {
  # Compute the length of the partitions
  dx <- (xr - xl) / nseg
  # Create equally spaced knots
  knots <- seq(xl - deg * dx, xr + deg * dx, by = dx)
  # Use bs() function to generate the B-spline basis
  get_bs_matrix <- matrix(bs(x, knots = knots, 
                             degree = deg, 
                             Boundary.knots = c(knots[1], knots[length(knots)])), 
                          nrow = length(x))
  # Remove columns that contain zero only
  bs_matrix <- get_bs_matrix[, -c(1:deg, ncol(get_bs_matrix):(ncol(get_bs_matrix) - deg))]

  return(bs_matrix)
}


# Observer list gets updated as new observers are added. 
# obs.list <- read.csv("Data/Observer list.csv", header = T) 
# colnames(obs.list) <- c("obs", "ID")
# seasons <- c("2006/2007", "2007/2008", "2009/2010", "2010/2011", 
#              "2014/2015", "2015/2016", "2019/2020", "2021/2022")

seasons <- c("2014/2015", "2015/2016", "2019/2020", "2021/2022")

n.seasons <- length(seasons)

# Load the most recent data that were newly extracted for the last four surveys:
Final_Data <- list()
k <- 3
for (k in 1:length(seasons)){
  tmp <- readRDS(paste0("RData/V2.1_Aug2022/out_",
                        strsplit(seasons[k], "/")[[1]][2],
                        "_Tomo_v2.rds"))

  Final_Data[[k]] <- tmp$Final_Data %>%
    mutate(Year = as.numeric(strsplit(seasons[k], "/")[[1]][2]),
           Day = as.numeric(BeginDay)) %>%
    right_join(., data.frame(Day = 1:90,
                             n = 0,
                             Year = as.numeric(strsplit(seasons[k], "/")[[1]][2])), 
               by = "Day") %>%

    select(Year.y, Day, n.x, n.y, dur, bf, vs, obs, begin) %>%
    mutate(n = ifelse(is.na(n.x), n.y, n.x),
           effort = ifelse(is.na(dur), 0, dur),
           Year = Year.y) %>%
    select(-c(Year.y, n.x, n.y, dur)) %>%
    arrange(Day) 
    #complete(Day, i) %>%
    #filter(!is.na(i))

}

Final_Data.all <- do.call(rbind, Final_Data)
Final_Data.all$St <- 1            # for this one, we only have 1 station (2022-08-19)

# Find all observers and change them into integer code:
unique.obs <- unique(Final_Data.all$obs)
unique.obs <- unique.obs[!is.na(unique.obs)]
obs.ID.df <- data.frame(obs = unique.obs,
                        obs.ID = 1:length(unique.obs))

Final_Data.all %>% 
  left_join(obs.ID.df, by = "obs") %>% 
  select(-obs) -> Final_Data.all

# figure out the number of periods
Final_Data.all %>% 
  group_by(Year) %>%
  #filter(effort > 0) %>%
  summarise(n = n()) -> periods

years <- unique(Final_Data.all$Year)
MCMC.params <- list(n.samples = 100000,
                    n.thin = 80,
                    n.burnin = 60000,
                    n.chains = 5)

```

The daily mean is a smooth function of days (1:90)

```{r data_setup}
Final_Data.all %>% 
  group_by(Year, Day) %>%
  summarise(daily.effort = sum(effort, na.rm = T)) %>%
  pivot_wider(values_from = daily.effort,
              names_from = Year,
              names_prefix = "Y")-> daily.effort

Final_Data.all %>% 
  group_by(Year, Day) %>%
  summarise(daily.counts = sum(n, na.rm = T)) %>%
  pivot_wider(values_from = daily.counts,
              names_from = Year,
              names_prefix = "Y")-> daily.counts

Final_Data.all %>% 
  group_by(Year, Day) %>%
  summarise(daily.effort = sum(effort, na.rm = T),
            daily.counts = sum(n, na.rm = T)) %>%
  mutate(daily.N = daily.counts/daily.effort) %>%
  select(-c(daily.effort, daily.counts)) %>%
  pivot_wider(values_from = daily.N,
              names_from = Year,
              names_prefix = "Y")-> daily.N

Final_Data.all %>% 
  group_by(Year, Day) %>%
  summarise(daily.obs = first(obs.ID)) %>%
  pivot_wider(values_from = daily.obs,
              names_from = Year,
              names_prefix = "Y")-> daily.obs

n.per.day <- obs.all <- n.all <- array(data = 0, 
               dim = c(max(periods$n), 2, length(seasons)))
days.all <- bf.all <- vs.all <- watch.prop <- matrix(data = 0,
                                                     nrow = max(periods$n), 
                                                     ncol = length(seasons))

for (k in 1:length(seasons)){
  n.all[1:periods$n[k],1, k] <- Final_Data[[k]]$n
  n.per.day[1:periods$n[k],1, k] <- Final_Data[[k]]$n/Final_Data[[k]]$effort
  bf.all[1:periods$n[k],k] <- Final_Data[[k]]$bf
  vs.all[1:periods$n[k],k] <- Final_Data[[k]]$vs
  # 540 minutes (maximum observation duration in a day) in unit of days
  watch.prop[1:periods$n[k],k] <- Final_Data[[k]]$effort/(540/24/60)
  days.all[1:periods$n[k],k] <- Final_Data[[k]]$Day
  
  obs.all[1:periods$n[k],1, k] <-   Final_Data.all %>% 
    filter(Year == years[k]) %>%
    select(obs.ID) %>% 
    pull() 
}

bf.all[is.na(bf.all)] <- 0
vs.all[is.na(vs.all)] <- 0
obs.all[is.na(obs.all)] <- max(obs.ID.df$obs.ID) + 1
n.per.day[is.na(n.per.day)] <- 0

```



```{r run-jags}
nseg <- 30

# time steps (1:90). Create the basis matrix
# bs_bbase is defined above.

X.j <- bs_bbase(1:90, nseg = nseg, deg = 3)

jags.params <- c("lambda","OBS.RF","OBS.Switch",
                "BF.Switch","BF.Fixed","VS.Switch",
                "VS.Fixed","mean.prob",
                "BF.Fixed",
                "VS.Fixed",
                "Corrected.Est","Raw.Est","N",
                "Daily.Est",
                "mu.N.beta", "sigma.N.beta",
                "sigma.N", "alpha", "r", "log.lkhd")

jags.data <- list(  n = n.all,
                    n.station = c(1,1,1,1),
                    n.year = length(seasons),
                    n.obs = nrow(obs.ID.df)+1,
                    periods = periods$n,
                    obs = obs.all,
                    vs = vs.all,
                    bf = bf.all,
                    watch.prop = watch.prop,
                    day = days.all,
                    X.j = X.j,
                    N_knots = ncol(X.j))

# 5 nodes produced errors; first error: Error in node N[63,4]
# Failure to calculate log density 
# Probably need to look at how nseg affects the outcome. Also,
# the variance of binomial distribution may be too small for the
# observed day-to-day variability. May try another distribution 2022-08-29

jm <- jags(jags.data,
             inits = NULL,
             parameters.to.save= jags.params,
             model.file = jags.model,
             n.chains = MCMC.params$n.chains,
             n.burnin = MCMC.params$n.burnin,
             n.thin = MCMC.params$n.thin,
             n.iter = MCMC.params$n.samples,
             DIC = T, parallel=T)

# Check conversions
mcmc_trace(jm$samples, c("Corrected.Est[1]", "Corrected.Est[2]"))


# This is the output file name
# jags.out.file <- paste0("RData/jagam_dailyN_k", 
#                         basis.dim, "_", family, "_jags.rds")

```


```{r}
mcmc_trace(jm$samples, c("Corrected.Est[1]", "Corrected.Est[2]",
                         "Corrected.Est[3]", "Corrected.Est[4]"))

```

