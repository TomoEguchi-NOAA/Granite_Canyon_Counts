---
title: "A new approach to gray whale abundance estimation"
author: "Tomo Eguchi"
date: "`r Sys.Date()`"
output: 
  bookdown::word_document2: default
bibliography: reference.bib
csl: marine-ecology-progress-series.csl
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
save.fig <- T

source("Granite_Canyon_Counts_fcns.R")
library(tidyverse)
library(lubridate)
library(flextable)
library(jagsUI)
library(bayesplot)
library(ggpubr)
library(R2WinBUGS)
library(abind)
library(rmarkdown)
library(loo)
library(bayesplot)
library(ggridges)
library(rstanarm)

options(mc.cores = 5)

set_flextable_defaults(font.size = 9,
                       font.family = "Cambria")

min.dur <- 60
dpi.set <- 600
WinBUGS.Run.Date <- "2025-04-11"

save.fig.fcn <- function(fig, file.name, replace = F, 
                         dpi = dpi.set, device = "png",
                         height = 3, width = 3, units = "in",
                         bg = "white"){
  if (isTRUE(replace) | (!isTRUE(replace) & !file.exists(file.name)))
    ggsave(fig, 
           filename = file.name, 
           dpi = dpi, device = device,
           height = height, width = width, units = units,
           bg = bg)
  
}


```

## Introduction {.unnumbered}

The Eastern North Pacific (ENP) gray whale (*Eschrichtius robustus*) population undertakes one of the longest annual migrations of any mammal, traveling between its Arctic feeding grounds and the warm-water lagoons of Baja California, Mexico, where they calve and breed. This iconic population has been the subject of extensive research due to its remarkable recovery from historic whaling and its continued vulnerability to environmental changes and anthropogenic impacts. Understanding the dynamics of the ENP gray whale population, including its abundance, distribution, and migratory patterns, is crucial for effective conservation and management efforts.

Analytical methods to estimate the abundance of gray whales from visual surveys at Granite Canyon, CA, have evolved over the years [@laakeGrayWhaleSouthbound2012; @durbanEstimatingGrayWhale2015]. Laake et al. (2012) used the distance sampling approach with generalized additive models (GAMs). Durban et al. (2015) developed a new method using a Bayesian N-mixture approach, which was approved by the IWC and it has been used for the analysis since the 2015/2016 season. In short, Durban et al.'s method used the observed changes in migrating gray whales at a location along the migration corridor. The general trend is that the number of gray whale sightings increases over time until it reaches its peak, then decreases. Durban et al.'s method used the Gaussian function to capture this general trend. Deviations from the Gaussian function was modeled via fitting a spline function to the observed counts.

The analysis was conducted using WinBUGS, which had become obsolete over the last decade or so. In this report, I provide improvements to Durban et al.'s method. In the following, I first briefly describe the method by Durban et al. and point out underlying assumptions. Then, I introduce a new approach that is consistent with the basic idea of the method by Durban et al., but improve it by replacing the Gaussian/spline model with another function. Finally, I reanalyze the data from Granite Canyon, CA, to compare abundance estimates among the three approaches.

## Method by Durban et al. {.unnumbered}

To estimate the detectability of gray whales by visual observers at the field station located at Granite Canyon, CA, counts from two independent stations of paired observers operating simultaneously were compared during two seasons (2009/2010 and 2010/2011). The two watch stations were positioned 35 m apart at the same elevation (22.5 m) above sea level (Durban et al. 2015). For the years with only one station, the detectability was extrapolated for all monitored watch periods based on the fitted model for the detectability, where the counts for the south watch station were treated as zero inflated binomial outcomes. The binomial probability was specified as the product of an indicator function and the detectability ($u_{i,j,t} \times p_{i,j,t}$), where $u$ = 1 or 0 to indicate whether or not count data were collected from that station, respectively. This formulation ensured that structural zero counts from periods without a second watch did not contribute to the likelihood for estimation of $p$ or $N$ (Durban et al. 2015).

Consistent with Laake et al. (2012), the model for the detectability incorporated fixed effects for visibility ($V$) and the Beaufort sea state ($B$), whereas observers were treated as random effects ($O$). These were modeled as additive effects on a general intercept so that the direction and magnitude of the estimated effects away from zero (no effect) could be assessed. The selection for the inclusion of these effects were accomplished by using Bayesian model selection with stochastic binary indicator variables $I$ to switch each of the three possible effects (i.e., $V$, $B$, and $O$) either in or out of the model.

A mathematical description of the approach by Durban et al. is in the Appendix.

### Some concerns about Durban et al's method {.unnumbered}

The approach in Durban et al. (2015) used the "cut" function within WinBUGS to dissociate estimated parameters in one function (i.e., $f_{d,t}$ or $g_{d,t}$) from those in the other function (i.e., $g_{d,t}$ or $f_{d,t}$, respectively). The function (\cut) is unavailable in modern Bayesian computation packages (e.g., JAGS, STAN) and its use has been questioned [@plummer2015]. In short, "cut" function does not converge to a well-defined limiting distribution (Plummer 2015). In addition, the assumption that the number of gray whales migrating in front of the observation station follows a Gaussian distribution centered around the mid point of the pre-defined migration season is somewhat questionable. The true curve in abundance may not be symmetric around a peak and the peak may not be instantaneous. In other words, the peak may persist for a few days. Fitting spline functions to observed counts may alleviate some of these problems but a fitted spline to observed counts may be too flexible. A spline fit would lose the general idea that the number of whales increases from the beginning of a migration season, reaches a peak, then decreases over time to the end of the migration season. In addition, Durban et al.'s approach implicitly assumed that two observation stations operated simultaneously with identical effort. In reality, however, watch durations were not identical between two stations in previous years (i.e., data in Laake's analysis).

## Modification {.unnumbered}

Because of the way spline is fit to data points, it is impossible to use just spline when data are not collected from day 1. Missing data points (e.g., weekends, not having 90 days of observations) are linearly interpolated unless there is an assumed data generating model (the Gaussian distribution in the approach by Durban et al.) that is common among years. Instead of Gaussian distribution and spline functions, I use one flexible function.

### Model description {.unnumbered}

In order to overcome these difficulties with the previous approach, I use a more flexible function that can accommodate the general shape (increase, peak then decrease) and asymmetrical around the peak. This pattern can be modeled with the product of two sigmoid equations or mathematical functions with a peak in the middle. Durban et al. used a Gaussian function to model this pattern. A similar pattern is often found in nesting marine turtles. Girondot et al. (2007) used the first-order derivative of a modified form of the Verhurst equation to model the number of nesting leatherback turtles in French Guiana and Suriname. The equation is similar to the Richards equation (Richards 1959) and has the following form.

$$M_1 = (1 + (2 e^K - 1) * e^{(P_1-d)/S_1}) ^ {(-1/e^K)}$$

$$M_2 = (1 + (2 e^K - 1) * e^{(P_2-d)/S_2}) ^ {(-1/e^K)}$$

$$N = N_{min} + (N_{max} - N_{min}) * (M_1 * M_2),$$

where $d > 0$ is the number of days since an arbitrary start date,

$S_1 < 0$ and $S_2 > 0$ define how the slope increases and decreases, respectively,

$K > 0$ defines the "flatness" at the peak of the function,

$P_1$ and $P_2$ ($P_1 <= P_2$) define where the peak is relative to the range of $d$, where $min(d) < P_1 <= P_2 < max(d)$,

$N_{min}$ is zero, i.e., the number of whales migrating outside of a migration season and,

$N_{max} >> N_{min}$. $N_{max}$ is not the maximum number of whales migrating per day but it is a parameter that may be fixed or estimated during the analysis.

Effects of these parameters on the shape of the function are described in the Appendix.

I created eight versions of the equation, varied by how $P_1$, $P_2$, $S_1$, and $S_2$ were defined (Table x). The $N_{max}$ parameter was assumed to be year specific and $K$ constant. Each parameter had a hyper-distribution of either a Gaussian, uniform, or gamma distribution. Uniform distributions were used for the prior distributions of hyperparameters (Appendix Table X).

This equation provided the daily mean number of gray whales passing through the observation point. The daily number of whales passing through the observation point was modeled with a Poisson distribution with the mean.

$$ N_{t, y} \sim POI(\bar{N}_{t,y}) $$

where $\bar{N}_{t,y}$ is the expected number of whales that migrate through the sampling area on the $t$-th day of the season $y$, which is modeled with the equation above.

The N-mixture model of Durban et al. was used to approximate the sampling process. Specifically, the observation probability $p_{i_t, s, y}$ was a function of the viewing conditions (the Beaufort sea state ($B$) and visibility code ($V$)), the overall mean probability of detection ($\beta_0$), observer random effects ($O$), and a watch duration offset ($L$). The watch duration offset was included to accommodate differing duration of observation shifts due to inclement conditions. For the observer random effects, because some observers lacked multiple observation periods, they were not useful as a random effect in the model. Consequently, I pooled all observers with less than 10 sightings as one “observer.” I removed the indicator function that selected covariates in Durban's approach. Whether or not each variable was needed would be apparent from the coefficient (i.e., $\beta_B$ and $\beta_V$).

$$ logit(p_{i_t, s, y}) = \beta_0 + \beta_{B} * B_{i_t, y} + \beta_{V} * V_{i_t, y} + O_{i_t, s, y} + log(L_{i_t, y})  $$

Observed counts were modeled with binomial distributions with the daily abundance and the detection probability.

$$ n_{i_t,s,y} \sim BIN(N_{t, y}, p_{i_t, s, y}) $$

where $n_{i_t, s, y}$ is the observed number of gray whales during the watch period $i$ of the $t$-th day of the season $y$ from the station $s$, $N_{t, y}$ is the number of gray whales that migrated through the sampling area during the $t$-th day of season $y$, and $p_{i_t, s, y}$ is the sighting probability of the station $s$ during the watch period $i$ of the $t$-th day of the season $y$.

The total number of gray whales for season $y$ is the sum of all $N_{t, y}$ and corrected for nighttime passage:

$$ N_y = \lambda * \sum_{t = 1} ^ {max(d)} N_{t, y} $$ and

$$ \lambda \sim N(1.0875, 0.03625) $$ (Perryman et al. 1999).

In Durban’s analysis, the annual migration started on December 1 and ended 90 days later, where the number of passing whales was assumed to be zero on days 1 and 90 (February 29th or March 1st). In previous years, however, observations were made beyond the 90th day. To analyze all data, I defined the annual migration period to be 100 days (March 10th or 11th), rather than 90 days, starting on December 1, and assumed that the number of passing whales was zero on days 1 and 100.

The models were fitted to data using JAGS (Plummer) in the R statistical environment, using the *jagsUI* package (Kellner 2024. Kellner K (2024). jagsUI: A Wrapper Around 'rjags' to Streamline 'JAGS' Analyses\_. R package version 1.6.2, <https://CRAN.R-project.org/package=jagsUI>.). Goodness-of-fit was determined via Pareto-k statistics ([@vehtari_practical_2017]) Vehtari et al. 2017), using the *loo* package (Vehtari et al. 2024, Vehtari A, Gabry J, Magnusson M, Yao Y, Bürkner P, Paananen T, Gelman A (2024). “loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models.” R package version 2.8.0, <https://mc-stan.org/loo/>.), and the convergence of MCMC was determined through the rank-normalized R-hat statistic (Vehtari et al. 2021). For each model, 550,000 steps of five chains were run, where the first 500,000 samples of each chain were treated as “burn-in” and discarded. The remaining 250,000 posterior samples (50,000 samples of five chains) were thinned by taking every 100 samples, resulting in 2500 posterior samples for making inference.

To compare the three approaches (Laake’s, Durban’s, and the new), I fitted generalized additive models to point abundance estimates for the last 11 years. Raw data were not available for the 2007/2008 season, so it was not included in Laake’s and the new analyses. Further, the rate of recent decline in the estimated abundance was compared among the three approaches by fitting linear models to the point abundance estimates between the 2015/2016 and 2024/2025 seasons. The new method was also compared to Laake's approach over the entire dataset (1967/1968 - 2024/2025).

JAGS models and R scripts to conduct the analysis, including data extraction and executing the JAGS models, are available at the GitHub repository (<https://github.com/TomoEguchi-NOAA/GrayWhaleAbundance>).

### Results {.unnumbered}

The entire dataset from the 1967/1968 season was analyzed using the new approach and the results compared to the reported estimates in Eguchi et al. (2025). Sightings data between the 1967/1968 and 2005/2006 seasons were obtained from the *ERAnalysis* package (<https://github.com/jlaake/ERAnalysis>, Laake 2012). An R script was written to convert the sightings data in the package to an input structure for the new analysis. No raw data were available for the 2006/2007 season. Consequently, it was excluded from the analysis. The remainder of the data, i.e., 2007/2008 to 2024/2025, were extracted from raw data files using an R script (available at the GitHub repository). All sighting data were used create input data for the new analysis, where all shifts of at least 60 minutes with the Beaufort sea state of \< 5 and the visibility code of \< 5 were included in the analysis.

```{r model-comparison, echo=FALSE, message=FALSE, warning=FALSE}
model.names <- c("2", "15", "17", "18",
                 "5", "16", "19", "20")
# Select only parameters for the function. 
# ^ means the subsequent letter is the beginning of a string
# \\. means a literal period
# | means "or"
# ^P\\b(?!\\.\\w*) means P followed by nothing, including a period or a white space
# \\[ means a literal bracket
# \\b is a word boundary
# (?!...) is a negative look ahead, which requires the perl = TRUE argument in R's grep.
params <- "^VS\\.Fixed|^BF\\.Fixed|^Max\\[|^S1\\[|^S2\\[|^P\\b(?!\\.\\w*)|^OBS\\.RF\\["

# For constant Max models (Model 7 and Model 8)
params.1 <- "^VS\\.Fixed|^BF\\.Fixed|^Max|^S1\\[|^S2\\[|^P\\b(?!\\.\\w*)|^OBS\\.RF\\["

# including hyperparameters and year-specific Max (all models except 7 and 8.
params.2 <- "^VS\\.Fixed|^BF\\.Fixed|^Max\\[|^S1|^S2|^P|^OBS\\.RF\\["

#max.Rhat.big <- list()
n.big.Rhat <- n.bad.Pareto <- prop.bad.Pareto <- LOOIC <- vector(mode = "numeric", length = length(model.names))

for (k in 1:length(model.names)){
  .out <- readRDS(paste0("RData/JAGS_Richards_Nmixture_v", 
                         model.names[k], "a_1968to2025_min", 
                         min.dur, "_NoBUGS.rds"))
  new.Rhat <- rank.normalized.R.hat(.out$jm$samples, params.2)
  #max.Rhat <- lapply(.out[[k]]$jm$Rhat, FUN = max, na.rm = T) %>%
  #  unlist()
  max.Rhat.big <- new.Rhat[which(new.Rhat > 1.01)]
  
  # data.array <- .out$jags.input$jags.data$n
  # data.array[,2,which(.out$jags.input$jags.data$n.station == 1)] <- NA
  # data.array[,2,which(.out$jags.input$jags.data$n.station == 1)] <- NA

  LOOIC.n <- compute.LOOIC(loglik.array = .out$jm$sims.list$log.lkhd,
                           #data.array = data.array,
                           MCMC.params = .out$MCMC.params)
  n.big.Rhat[k] <- length(max.Rhat.big)
  LOOIC[k] <- LOOIC.n$loo.out$estimates["looic", "Estimate"]
  n.bad.Pareto[k] <- sum(LOOIC.n$loo.out$pointwise[,5] > 0.7)
  prop.bad.Pareto[k] <- 100 * (n.bad.Pareto[k]/nrow(LOOIC.n$loo.out$pointwise))
}

out.table <- data.frame(model = 1:length(model.names),
                        #LOOIC = LOOIC,
                        dLOOIC = LOOIC - min(LOOIC),
                        n.big.Rhat = n.big.Rhat,
                        n.bad.Pareto = n.bad.Pareto,
                        p.bad.Pareto = prop.bad.Pareto) %>% arrange(by = dLOOIC)

```

The eight models were fitted to the sightings data over 32 seasons. According to the rank-normalized R-hat statistic, Model `r out.table$model[which(n.big.Rhat == min(out.table$n.big.Rhat))]` showed the least number (`r out.table$n.big.Rhat[which(n.big.Rhat == min(out.table$n.big.Rhat))]`) of the parameters with the rank-normalized Rhat \> 1.01. LOOIC indicated that Model `r out.table$model[which(out.table$dLOOIC == 0)]` was the best. According to the Pareto k statistics, Model `r out.table$model[which(out.table$n.bad.Pareto == min(out.table$n.bad.Pareto))]` showed the least number of data points that were considered ill fit (Appendix Table y). Considering these results, and visual inspection of trace plots of parameter that had the rank-normalized Rhat statistics \> 1.01 (Appendix Figure x), I concluded that Model 5 was the best, which consisted of year-specific P (P1 = P2), and year-specific S1 and S2 parameters (Table \@ref(tab:model-table)). In the following, I use Model 5 for parameter inference. 

```{r model-table, echo=F, message=F, tab.cap="Comparison of eight models."}
knitr::kable(out.table, col.names = c("Model", "$\\Delta$LOOIC", "# Rhat > 1.01", "# Pareto-K > 0.7", "% Pareto-K > 0.7" ), digits = 2, escape = FALSE)

```


```{r JAGS-results, echo=F,message=FALSE}
rm(list = c(".out"))
ver <- 5
jm.out <- readRDS(paste0("RData/JAGS_Richards_Nmixture_v",
                         ver,
                         "a_1968to2025_min", min.dur,
                         "_NoBUGS.rds"))

# LOOIC and Pareto-k stats
LOOIC.n <- compute.LOOIC(loglik.array = jm.out$jm$sims.list$log.lkhd,
                         MCMC.params = jm.out$MCMC.params)

# Match Pareto-k and data. Note that days 1 and 100 should be removed.
n.array <- jm.out$jags.input$jags.data$n
days <- jm.out$jags.input$jags.data$day

# Create an array with corresponding years in non-NA elements
year.array <- array(dim = dim(n.array))
all.start.years <- c(jm.out$jags.input$jags.input.Laake$all.start.year,
                     jm.out$jags.input$jags.input.new$start.years)

for (k in 1:length(all.start.years)){
    year.array[, , k] <- all.start.years[k]
}

n.vec <- n.array[days > 1 & days < 100] %>% na.omit()
day.vec <- days[days > 1 & days < 100] %>% na.omit()
year.vec <- year.array[days > 1 & days < 100] %>% na.omit()

Pareto.df <- data.frame(year = year.vec,
                        day = day.vec,
                        n = n.vec,
                        Pareto.K = LOOIC.n$loo.out$diagnostics$pareto_k)

# trace plots of parameters with high Rhat values:
new.Rhat <- rank.normalized.R.hat(jm.out$jm$samples, params.2)
max.Rhat.big <- new.Rhat[which(new.Rhat > 1.01)]
params.Rhat.big <- names(max.Rhat.big)

p.trace.1 <- mcmc_trace(jm.out$jm$samples, 
                        pars = params.Rhat.big[1:25])

p.trace.2 <- mcmc_trace(jm.out$jm$samples, 
                        pars = params.Rhat.big[26:50])

p.trace.3 <- mcmc_trace(jm.out$jm$samples, 
                        pars = params.Rhat.big[51:58])

if (save.fig) {
  save.fig.fcn(fig = p.trace.1, 
               file.name = "figures/M5_trace_1.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 10, width = 10, units = "in",
               bg = "white")
  
  save.fig.fcn(fig = p.trace.2, 
               file.name = "figures/M5_trace_2.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 10, width = 10, units = "in",
               bg = "white")

  save.fig.fcn(fig = p.trace.3, 
               file.name = "figures/M5_trace_3.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 10, width = 10, units = "in",
               bg = "white")
}

all.start.year <- c(jm.out$jags.input$jags.input.Laake$all.start.year,
                    jm.out$jags.input$jags.input.new$start.years)

# Create a dataframe with all years, including unsampled years.
all.years <- data.frame(start.year = seq(min(all.start.year), max(all.start.year))) %>%
  mutate(Season = paste0(start.year, "/", start.year + 1))

# Look at the annual abundance estimates:
Nhat. <- data.frame(Season = paste0(all.start.year, "/",
                                    all.start.year+1),
                    Nhat = jm.out$jm$q50$Corrected.Est,
                    LCL = jm.out$jm$q2.5$Corrected.Est,
                    UCL = jm.out$jm$q97.5$Corrected.Est) %>%
  right_join(all.years, by = "Season") %>%
  arrange(start.year) %>%
  mutate(Method = paste0("Eguchi ", ver))

# Create an observed counts dataframe
obs.day <- jm.out$jags.input$jags.data$day[,1,]
obs.n <- jm.out$jags.input$jags.data$n[,1,]

obs.n.df <- data.frame(start.year = rep(all.start.year, each = 100),
                       Season = rep(paste0(all.start.year, "/",
                                           all.start.year+1),
                                    each = 100),
                       Day = rep(1:100, times = length(all.start.year)),
                       n = NA)

k1 <- k2 <- 1
for (k1 in 1:length(all.start.year)){
  obs.day.1 <- obs.day[, k1]
  obs.n.1 <- obs.n[, k1]
  obs.day.1.uniq <- na.omit(unique(obs.day.1))
  for (k2 in 1:length(obs.day.1.uniq)){
    obs.n.df[obs.n.df$start.year == all.start.year[k1] &
               obs.n.df$Day == obs.day.1.uniq[k2], "n"] <- sum(obs.n[obs.day[,k1] == obs.day.1.uniq[k2], k1], na.rm = T)
    
  }
  
}

# This is for daily estimates
N.hats.day <- data.frame(Season = rep(paste0(all.start.year, 
                                             "/", all.start.year+1),
                                      
                                      each = nrow(jm.out$jm$mean$N)), 
                         start.year = rep(all.start.year,
                                          each = nrow(jm.out$jm$mean$N)),
                         Day = rep(1:nrow(jm.out$jm$mean$N), 
                                   times = length(all.start.year)),
                         Mean = as.vector(jm.out$jm$mean$N),
                         LCL = as.vector(jm.out$jm$q2.5$N),
                         UCL = as.vector(jm.out$jm$q97.5$N),
                         obs.n = obs.n.df$n,
                         Method = "Eguchi") 

# To look at which data points had high Pareto-k values, I need to make all data 
# in series. 


# Daily estimates plots
p.daily.Richards <- ggplot(N.hats.day %>% group_by(Season)) + 
  geom_ribbon(aes(x = Day, ymin = LCL, ymax = UCL),
              fill = "blue", alpha = 0.5) +
  geom_path(aes(x = Day, y = Mean)) + 
  geom_point(aes(x = Day, y = obs.n), color = "darkgreen") +
  facet_wrap(~ Season)

save.fig.fcn(fig = p.daily.Richards, 
               file.name = "figures/Daily_Eguchi.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 5, width = 7, units = "in",
               bg = "white")

Reported.estimates <- read.csv(file = "Data/Nhats_2025.csv") %>%  
  transmute(Season = Season,
            Nhat = Nhat,
            LCL = LCL,
            UCL = UCL,
            Method = paste0(Method, "-Reported")) %>%
  right_join(all.years, by = "Season") %>%
  arrange(start.year) %>%
  relocate(Method, .after = start.year)
```

```{r Winbugs-results, echo=F,message=FALSE}
#WinBugs.run.date <- "2025-04-11"
WinBugs.out <- readRDS(file = paste0("RData/WinBUGS_2007to2025_v2_min", 
                                     min.dur,
                                     "_100000_",
                                     WinBUGS.Run.Date, ".rds"))

# Compute rank-normalized Rhat for WinBUGS output
BUGS.params <- "^lambda|^beta|^OBS.RF"
BUGS.samples <- WinBugs.out$BUGS.out$sims.array
BUGS.col.names <- grep(BUGS.params, dimnames(BUGS.samples)[[3]], 
                       value = T, perl = T)
subset.BUGS.samples <- BUGS.samples[,,BUGS.col.names]
subset.BUGS.mcmc.array <- as_draws_array(subset.BUGS.samples,
                                         .nchains = 5)
BUGS.Rhat <- apply(subset.BUGS.mcmc.array, MARGIN = 3, FUN = rhat)

#max.Rhat <- lapply(.out[[k]]$jm$Rhat, FUN = max, na.rm = T) %>%
#  unlist()
BUGS.Rhat.big <- BUGS.Rhat[which(BUGS.Rhat > 1.01)]

Corrected.Est <- WinBugs.out$BUGS.out$sims.list$Corrected.Est

# Create an observed counts dataframe
Bugs.obs.day <- WinBugs.out$BUGS.input$data$day
Bugs.obs.n <- WinBugs.out$BUGS.input$data$n[,1,]
Bugs.obs.n <- rbind(Bugs.obs.n, matrix(nrow = 2, ncol = ncol(Bugs.obs.n)))

Bugs.start.year <- WinBugs.out$BUGS.input$all.years - 1
Bugs.obs.n.df <- data.frame(start.year = rep(Bugs.start.year, 
                                             each = 90),
                       Season = rep(paste0(Bugs.start.year, "/",
                                           Bugs.start.year + 1),
                                    each = 90),
                       Day = rep(1:90, times = length(Bugs.start.year)),
                       n = NA)

k1 <- k2 <- 1
for (k1 in 1:length(Bugs.start.year)){
  obs.day.1 <- Bugs.obs.day[, k1]
  obs.n.1 <- Bugs.obs.n[, k1]
  obs.day.1.uniq <- na.omit(unique(obs.day.1)) 
  obs.day.1.uniq <- obs.day.1.uniq[obs.day.1.uniq > 1 & obs.day.1.uniq < 90]
  for (k2 in 1:length(obs.day.1.uniq)){
    Bugs.obs.n.df[Bugs.obs.n.df$start.year == Bugs.start.year[k1] &
                    Bugs.obs.n.df$Day == obs.day.1.uniq[k2], "n"] <- sum(Bugs.obs.n[Bugs.obs.day[,k1] == obs.day.1.uniq[k2], k1], 
                                                                         na.rm = T)
    
  }
  
}

# We don't have raw data for 2006/2007 and 2007/2008 seasons
seasons <- c("2006/2007", "2007/2008",
             jm.out$jags.input$jags.input.new$seasons)

all.season <- paste0(all.start.year, "/", all.start.year+1)
Durban.abundance.df <- data.frame(Season = WinBugs.out$BUGS.input$seasons,
                                  Nhat = apply(Corrected.Est,
                                               FUN = mean,
                                               MARGIN = 2),
                                  # CV = apply(Corrected.Est,
                                  #            FUN = function(x) 100*sqrt(var(x))/mean(x),
                                  #            MARGIN = 2),
                                  # median = apply(Corrected.Est, 
                                  #                FUN = median, 
                                  #                MARGIN = 2),
                                  LCL = apply(Corrected.Est, 
                                              MARGIN = 2, 
                                              FUN = quantile,
                                              0.025),
                                  UCL = apply(Corrected.Est, 
                                              MARGIN = 2, 
                                              FUN = quantile,
                                              0.975)) %>%
  right_join(all.years, by = "Season") %>%
  arrange(start.year) %>%
  mutate(Method = "Durban")

# Create a dataframe for daily estimates:
daily.estim <- WinBugs.out$BUGS.out$sims.list$Daily.Est

# get stats:
mean.mat <- LCL.mat <- UCL.mat <- matrix(data = NA, 
                                         nrow = dim(daily.estim)[2], 
                                         ncol = dim(daily.estim)[3])

for (k1 in 1:dim(daily.estim)[2]){
  for (k2 in 1:dim(daily.estim)[3]){
    mean.mat[k1, k2] <- mean(daily.estim[,k1,k2])
    LCL.mat[k1, k2] <- quantile(daily.estim[,k1,k2], 0.025)
    UCL.mat[k1, k2] <- quantile(daily.estim[,k1,k2], 0.975)
  }
  
}

N.hats.day.Durban <- data.frame(Season = rep(WinBugs.out$BUGS.input$seasons, 
                                             each = dim(daily.estim)[2]),
                                start.year = rep(WinBugs.out$BUGS.input$all.years - 1,
                                                 each = dim(daily.estim)[2]),
                                Day = rep(1:dim(daily.estim)[2], 
                                          length(WinBugs.out$BUGS.input$seasons)),
                                Mean = as.vector(mean.mat),
                                LCL = as.vector(LCL.mat),
                                UCL = as.vector(UCL.mat),
                                obs.n = Bugs.obs.n.df$n, 
                                Method = "Durban")
```

```{r all-results, message=FALSE, echo=FALSE}

N.hats.day.all <- rbind(N.hats.day, N.hats.day.Durban) %>% 
  filter(start.year > 2005)

# Daily estimates plots
p.daily.Durban <- ggplot(N.hats.day.Durban %>% group_by(Season)) + 
  geom_ribbon(aes(x = Day, ymin = LCL, ymax = UCL),
              fill = "blue", alpha = 0.5) +
  geom_path(aes(x = Day, y = Mean)) + 
  geom_point(aes(x = Day, y = obs.n), color = "darkgreen") +
  facet_wrap(~ Season)

if (save.fig)
  save.fig.fcn(fig = p.daily.Durban, 
               file.name = "figures/Daily_Durban.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 5, width = 7, units = "in",
               bg = "white")

p.daily. <- ggplot(N.hats.day.all %>% group_by(Season)) + 
  geom_ribbon(aes(x = Day, ymin = LCL, ymax = UCL, fill = Method),
              alpha = 0.5) +
  geom_path(aes(x = Day, y = Mean, color = Method), size = 1) + 
  geom_point(aes(x = Day, y = obs.n, color = Method)) +
  facet_wrap(~ Season)

if (save.fig)
  save.fig.fcn(fig = p.daily., 
               file.name = "figures/Daily_Eguchi_Durban.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 5, width = 7, units = "in",
               bg = "white")

# Include non-survey years - no estimates for 2007/2008 because I don't have
# raw data for that year. Only the WinBUGS inputs. 
Laake.abundance.new <- read.csv(file = "Data/all_estimates_Laake_2025.csv") %>%
  mutate(LCL = CL.low,
         UCL = CL.high) %>%
  select(c(Season, Nhat, LCL, UCL)) %>%
  right_join(all.years, by = "Season") %>%
  arrange(start.year) %>%
  mutate(Method = "Laake")

#Laake.output <- read_rds(file = "RData/Laake_abundance_estimates_2024.rds")

# In reported estimates, there are two 2006/2007.
Reported.estimates %>%
  na.omit() %>%
  select(Season) %>% 
  unique() -> sampled.seasons 

# Reported estimates are identical to the reanalysis so remove. 
Laake.abundance.new %>%
  rbind(Durban.abundance.df) %>%
  rbind(Nhat.) -> all.estimates
#  rbind(spline.Nhat) 
  #rbind(Reported.estimates %>% na.omit()) -> all.estimates

p.Nhats <- ggplot(all.estimates) +
  geom_point(aes(x = start.year, y = Nhat,
                 color = Method),
             alpha = 0.5) +
  geom_errorbar(aes(x = start.year, ymin = LCL, ymax = UCL,
                    color = Method)) +
  ylim(5000, 35000) +
  xlab("") + ylab("Abundance") +
  theme(legend.position = "top")

if (save.fig)
  save.fig.fcn(fig = p.Nhats,
               file.name = paste0("figures/Nhats_v", 
                                  ver, "_min", min.dur, 
                                  "_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)

Nhat. %>% 
  select(Season, start.year, Nhat, LCL, UCL) %>%
  rename(Nhat.Eguchi = Nhat,
         LCL.Eguchi = LCL,
         UCL.Eguchi = UCL) %>%
  cbind(Laake.abundance.new %>%
          select(Nhat, LCL, UCL) %>%
          rename(Nhat.Laake = Nhat,
                 LCL.Laake = LCL,
                 UCL.Laake = UCL)) %>%
  cbind(Durban.abundance.df %>%
          select(Nhat, LCL, UCL) %>%
          rename(Nhat.Durban = Nhat,
                 LCL.Durban = LCL,
                 UCL.Durban = UCL)) %>%
  mutate(d.Laake.Eguchi = Nhat.Laake - Nhat.Eguchi,
         d.Durban.Eguchi = Nhat.Durban - Nhat.Eguchi) -> Nhat.all.wide


# Compare how daily sums among years
obsd.periods.primary <- jm.out$jags.input$jags.data$periods[,1]
watch.prop.primary <- jm.out$jags.input$jags.data$watch.prop[,1,]
obsd.effort.primary <- rbind(rep(0, times = dim(watch.prop.primary)[2]), 
                             watch.prop.primary, 
                             rep(0, times = dim(watch.prop.primary)[2]))

obsd.n.primary <- jm.out$jags.input$jags.data$n[,1,]
obsd.day.primary <- jm.out$jags.input$jags.data$day[,1,]
obsd.n.prop <- obsd.n.primary[,] * obsd.effort.primary

obsd.n.df <- data.frame(Season = rep(all.season, each = dim(obsd.n.prop)[1]),
                        obsd.n = as.vector(obsd.n.prop),
                        day = as.vector(obsd.day.primary),
                        effort = as.vector(obsd.effort.primary)) %>%
  na.omit()

# ggplot(obsd.n.df) +
#   geom_point(aes(x = day, y = obsd.n)) +
#   facet_wrap(~ Season)
# 
# ggplot(obsd.n.df) +
#   geom_point(aes(x = day, y = effort)) +
#   facet_wrap(~ Season)

p.RF.Obs <- plot.trace.dens(jm.out$jm, "OBS.RF")

if (save.fig){  
  save.fig.fcn(fig = p.RF.Obs$p.trace, 
               file.name = "figures/Observer_trace.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 5, width = 7, units = "in",
               bg = "white")
  
  save.fig.fcn(fig = p.RF.Obs$p.dens, 
               file.name = "figures/Observer_dens.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 5, width = 7, units = "in",
               bg = "white")
  
}
```

Estimated abundances from the new approach generally followed the same trend as the previous analyses except a few notable differences (Figure \@ref(fig:Figure-N-hats)). Laake’s and the new approaches exhibited lower estimates than those from Durban’s approach, where the new approach resulted in the lowest. The exact abundance estimates, however, are not critical because they do not reflect the total abundance of the population. Patterns of fluctuations and trends in estimates are more important than the point estimates. The three approaches exhibited similar patterns and fluctuations in estimated abundance.

```{r Figure-N-hats, echo=FALSE, message=FALSE, fig.cap = "Estimated annual abundance and their 95% CIs using Laake's method (green), Durban's method (red), and new approach (blue)."}

knitr::include_graphics(paste0("figures/Nhats_v", 
                               ver, "_min", min.dur, "_", dpi.set, "dpi.png"))

```

Posterior distributions of the parameters of the Richards function provided valuable insights into how annual gray whale migration varied among years. The P parameters, which defined the location of the peak, changed over the years (Figure \@ref(fig:Figure-P-posteriors)). This has been reported in previous studies (Eguchi et al. 2023, 2024, 2025), although the significance and causes for the change are unknown. The S1 and S2 parameters, which define the width of the function, also varied among years, indicating that the duration of migration varied among years (Appendix Figure y).

```{r posterior-Richards-plots, echo=FALSE, message=FALSE}

posteriors.P <- plot.trace.dens(jm.out$jm, 
                                var.name = "P")

posteriors.P$df %>%
  mutate(start.year = all.start.year[numeric.index],
         start.year.f = as.factor(start.year)) %>% 
  rename(P = sample)-> P.df

p.posteriors.P <- ggplot(P.df, aes(x = P, y = start.year.f)) +
  geom_density_ridges() +
  ylab("")

if (save.fig)
  save.fig.fcn(fig = p.posteriors.P,
               file.name = paste0("figures/posterior_P_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)

# Need to include \\[ to remove S1.alpha and S1.beta
posteriors.S1 <- plot.trace.dens(jm.out$jm,
                                 var.name = "S1\\[")

posteriors.S1$df %>%
  mutate(start.year = all.start.year[numeric.index],
         start.year.f = as.factor(start.year)) %>% 
  rename(S1 = sample)-> S1.df

p.posteriors.S1 <- ggplot(S1.df, aes(x = S1, y = start.year.f)) +
  geom_density_ridges() +
  ylab("")

if (save.fig)
  save.fig.fcn(fig = p.posteriors.S1,
               file.name = paste0("figures/posteriors_S1_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)
 
posteriors.S2 <- plot.trace.dens(jm.out$jm,
                                 var.name = "S2\\[")

posteriors.S2$df %>%
  mutate(start.year = all.start.year[numeric.index],
         start.year.f = as.factor(start.year)) %>% 
  rename(S2 = sample)-> S2.df

p.posteriors.S2 <- ggplot(S2.df, aes(x = S2, y = start.year.f)) +
  geom_density_ridges() +
  ylab("")

if (save.fig)
  save.fig.fcn(fig = p.posteriors.S2,
               file.name = paste0("figures/posteriors_S2_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)


```

```{r Figure-P-posteriors, echo=FALSE, message=FALSE, fig.cap = "Posterior distributions of year-specific P parameters."}

knitr::include_graphics(paste0("figures/posterior_P_", dpi.set, "dpi.png"))

```

```{r posterior-fixed-effects-plots, echo=FALSE, message=FALSE}

posteriors.Fixed <- plot.trace.dens(jm.out$jm, 
                                    var.name = ".Fixed")

p.posteriors.Fixed <- ggplot(posteriors.Fixed$df) +
  geom_density(aes(x = sample)) +
  facet_wrap(~ par.name.ordered, scales = "free_x") +
  xlab("") + ylab("Density")
  

if (save.fig)
  save.fig.fcn(fig = p.posteriors.Fixed,
               file.name = paste0("figures/posterior_Fixed_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)

# Look into why BF.Fixed is negative.
n.mat <- jm.out$jags.input$jags.data$n[,1,]
bf.mat <- rbind(matrix(data = NA, nrow = 1, ncol = length(all.start.year)),
                       jm.out$jags.input$jags.data$bf[,1,],
                matrix(data = NA, nrow = 1, ncol = length(all.start.year)))
                
day.mat <- jm.out$jags.input$jags.data$day[,1,]

n.and.bf <- data.frame(n = n.mat[day.mat > 1 & day.mat < 100],
                       bf = bf.mat[day.mat > 1 & day.mat < 100])

```

The fixed effect of the Beaufort sea state on observation probability was positive (Figure \@ref(fig:Figure-Fixed-posteriors)) indicating that higher Beaufort sea states corresponded with greater detection probabilities. This was a perplexing result. More singleton whales might have been detected at low sea states than at higher sea states, whereas only large groups were sighted at high sea states. At the Beaufort sea state 0, the largest observed group was `r n.and.bf %>% filter(bf == 0) %>% summarise(max.n = max(n))`, whereas there were `r n.and.bf %>% filter(bf > 0, n > 74) %>% summarise(n.more.than.74 = n())` groups that were greater than 74 for the Beaufort sea state \> 0. As expected, the fixed effect of the sighting condition on observation probability was negative (Figure \@ref(fig:Figure-Fixed-posteriors)), indicating that less favorable conditions resulted in lower detection probabilities. The observer random effects indicated significant differences among observers, as found in the previous analysis by Durban et al. (Appendix Figure x).

```{r Figure-Fixed-posteriors, echo=FALSE, message=FALSE, fig.cap = "Posterior distributions of the fixed-effects coefficients for the visibility code (VS) and Beaufort sea state (BF)."}

knitr::include_graphics(paste0("figures/posterior_Fixed_", dpi.set, "dpi.png"))

```

The daily estimated mean between Durban's and the new approaches was similar qualitatively (Figure \@ref(fig:Figure-daily-N-hats)), indicating the two approaches used the data in a similar manner. 

```{r Figure-daily-N-hats, echo=FALSE, message=FALSE, fig.cap = "Estimated daily abundance and their 95% CIs using Durban's (red) and the new approach (green). The solid lines indicate the mean. For the 2006/2007 season, the new approach was applied to the data from the *ERAnalysis* package, whereas WinBUGS was ran on the input structure in Durban's analysis with 90 minutes as the minimum shift duration. Consequently, the observed counts are different between the two approaches. For the 2007/2008 season, no raw data were available. Therefore, the new approach was not applied to the data."}

knitr::include_graphics("figures/Daily_Eguchi_Durban.png")
```


```{r gam-glm-fit, echo=F,message=FALSE}
library(mgcv)
all.estimates %>% na.omit() %>%
  mutate(time = start.year - min(start.year)) -> all.estimates.gam
  
all.estimates.gam %>%
  filter(Method == "Eguchi 5") %>% 
  filter(start.year > 2005) -> gam.estimates.Eguchi

all.estimates.gam %>% 
  filter(Method == "Laake") %>% 
  filter(start.year > 2005) -> gam.estimates.Laake

all.estimates.gam %>%
  filter(Method == "Durban") %>% 
  filter(start.year > 2005)-> gam.estimates.Durban

gam.Eguchi <- mgcv::gam(Nhat ~ s(time), 
                        method = "REML", 
                        data = gam.estimates.Eguchi )
gam.Laake <- mgcv::gam(Nhat ~ s(time), 
                       method = "REML",
                       data = gam.estimates.Laake)
gam.Durban <- mgcv::gam(Nhat ~ s(time), 
                        method = "REML",
                        data = gam.estimates.Durban)

pred.data <- data.frame(time = seq(min(gam.estimates.Eguchi$time), 
                                   max(gam.estimates.Eguchi$time), length.out = 200))

gam.preds.Eguchi <-predict(gam.Eguchi, newdata = pred.data, se.fit = TRUE)
gam.preds.Durban <-predict(gam.Durban, newdata = pred.data, se.fit = TRUE)
gam.preds.Laake <-predict(gam.Laake, newdata = pred.data, se.fit = TRUE)

# Combine predictions with the x values and calculate the confidence interval
pred.data %>%
  mutate(fit = gam.preds.Eguchi$fit,
         SE = gam.preds.Eguchi$se.fit,
         upper_CI = gam.preds.Eguchi$fit + (1.96 * SE),
         lower_CI = gam.preds.Eguchi$fit - (1.96 * SE),
         Method = "Eguchi") -> pred.Eguchi

pred.data %>%
  mutate(fit = gam.preds.Durban$fit,
         SE = gam.preds.Durban$se.fit,
         upper_CI = gam.preds.Durban$fit + (1.96 * SE),
         lower_CI = gam.preds.Durban$fit - (1.96 * SE),
         Method = "Durban") -> pred.Durban

pred.data %>%
  mutate(fit = gam.preds.Laake$fit,
         SE = gam.preds.Laake$se.fit,
         upper_CI = gam.preds.Laake$fit + (1.96 * SE),
         lower_CI = gam.preds.Laake$fit - (1.96 * SE),
         Method = "Laake") -> pred.Laake

pred.all <- rbind(pred.Eguchi, pred.Durban, pred.Laake) 
  #mutate(method.f = as.factor(method))

p.gam.fit <- ggplot() +
  # Add the confidence interval ribbon using the new prediction data
  geom_ribbon(data = pred.all,
              aes(x = time, ymin = lower_CI, ymax = upper_CI, fill = Method), 
              alpha = 0.3) +
  
  # Add the fitted line using the new prediction data
  geom_line(data = pred.all,
            aes(x = time, y = fit, color = Method), 
            size = 1) +
  scale_x_continuous(labels = function(breaks){ paste0(breaks + 1967)}) +
  xlab("") + ylab("Abundance") +
  theme(legend.position = "top")
  
  # geom_point(data = all.estimates.gam %>%
  #              filter(start.year > 2005),
  #            aes(x = time, y = Nhat, color = Method)) +  # Plot the raw data points from the original dataframe
  

if (save.fig)
  save.fig.fcn(fig = p.gam.fit,
               file.name = paste0("figures/gam_fit_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)

lm.Eguchi <- lm(log(Nhat) ~ time, 
                data = all.estimates.gam %>% filter(start.year > 2013, Method == "Eguchi 5"))

lm.Laake <- lm(log(Nhat) ~ time, 
               data = all.estimates.gam %>% filter(start.year > 2013, Method == "Laake"))

lm.Durban <- lm(log(Nhat) ~ time, 
                data = all.estimates.gam %>% filter(start.year > 2013, Method == "Durban"))

```

Fitted GAMs to the estimated abundance between the 2006/2007 and 2024/2025 seasons indicated that the three approaches provided comparable results in terms of the general trend (Figure \@ref(fig:Figure-gam-fits)). Furthermore, linear models ($log(Nhat) = time + \epsilon$) fitted to the point estimates from the 2014/2015 season to the 2024/2025 season indicated a statistically indistinguishable annual rate of decline among the three approaches (Laake: `r signif(lm.Laake$coefficients[2],1)` $\pm$ `r signif(coef(summary(lm.Laake))["time", "Std. Error"], 2)`. Durban: `r signif(lm.Durban$coefficients[2],1)` $\pm$ `r signif(coef(summary(lm.Durban))["time", "Std. Error"], 2)`, Eguchi: `r signif(lm.Eguchi$coefficients[2],1)` $\pm$ `r signif(coef(summary(lm.Eguchi))["time", "Std. Error"], 2)`).

```{r Figure-gam-fits, echo=FALSE, message=FALSE, fig.cap = "Fitted generalized additive models to Estimated abundance and their 95% CIs."}

knitr::include_graphics(paste0("figures/gam_fit_", dpi.set, "dpi.png"))
```

## Discussion {.unnumbered}

The new approach yielded comparable results to those obtained by Laake’s and Durban’s approaches. Although point estimates were different in some years, confidence/credible intervals overlapped in majority of the time series. The new approach included several favorable properties compared with Durban’s approach. First, it eliminated the use of the “cut” function, which had been deemed statistically inappropriate (Plummer yr). Second, it is coded in a more contemporary language (JAGS), which is widely used and freely available. Third, the goodness-of-fit is determined through a modern criterion using log-likelihood (LOOIC and Pareto-k statistic). In Durban’s approach, only one model was available, and the goodness-of-fit was not evaluated. Fourth, it runs faster than WinBUGS. The most recent analysis over the past 11 years using WinBUGS with 100,000 MCMC steps took `r signif(WinBugs.out$Run_Time, 3)` hrs. In comparison, the entire 32-year dataset was analyzed in approximately `r signif(jm.out$Run_Time, 2)` hrs on the same computer using the new approach with 550,000 MCMC steps of five chains using JAGS. Fifth, although the general modeling framework is the same N-mixture model, the new approach is simpler.

GAMs fitted to point estimates of abundance indicated that these models provided a similar pattern of fluctuations in the EP gray whale abundance. The rate of recent decline in abundance was the same regardless of the approach used. These results showed that the new approach provided equivalent abundance estimates to the previous approaches. 

The new approach can accommodate additional information if it becomes available. For example, there is an effort to estimate the precision and possible biases of estimated group sizes by visual observers, using uncrewed aircraft systems (UAS). Suppose a correction function can be determined for each sighting from the UAS-based data, conditional on the recorded group size by visual observers. Such a function can be incorporated into the observation likelihood loop, where each sighting within a period is corrected using the function. The model can be incorporated into a holistic population model, such as one created by Stewart et al. (2023). 

In summary, the new approach provided a simpler and faster algorithm to estimate abundance of gray whales from the long-term visual surveys conducted at Granite Canyon, CA. 

## Literature cited {.unnumbered}

Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P.-C. (2021). Rank-normalization, folding, and localization: An improved R-hat for assessing convergence of MCMC. Bayesian Analysis, 16(2), 667–718. <https://doi.org/10.1214/20-BA1221>

Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). <https://doi.org/10.1007/s11222-016-9696-4>)

## Appendix {.unnumbered}

### Mathematical description of Durban's approach {.unnumbered}

This section was extracted from Durban et al. (2015), almost verbatim. I edited some places to make explanations clearer and changed some symbols to make them consistent between theirs and the new approach.

The total counts of whales ($n_{i,d(j),t}$) during the watch period $j$ of the $d$th day in the year $t$ at the watch station $i$ was modeled as a binomial random deviate (in the paper, $j$ was not specified as the $j$th watch period during the $d$th day of the season):

$$ n_{i,d(j),t} \sim BIN(N_{d(j),t}, p_{i,d(j),t}). $$

The binomial $N_{d(j),t}$ parameter is the unknown total number of gray whales passing through the study area during the watch period $j$ in the $d$th day in the year $t$.

The detection probability was modeled as a function of visibility (V), Beaufort scale (B), and observers (O).

$$ logit(p_{i,d(j),t}) = logit(p_0) + I_{V} \beta_{V} V_{d(j),t} + I_{B} \beta_{B} B_{d(j),t}+ I_{O} \mathbf{\beta}^{O_{i,d(j),t} = o}_{O} $$

where the intercept $p_0$ was the base detection probability in the absence of covariate effects, assigned a Uniform(0,1) prior distribution, and $logit(p_0) = ln(p_0/(1–p_0))$. For each fixed effect, $\beta_{B}$ and $\beta_{V}$, a Normal prior distribution with mean zero and standard deviation of 10 was used. The random effect for each observer was drawn from a Normal distribution with mean zero and standard deviation $\sigma_{O} \sim Uniform(0,10)$. Each binary indicator variable, I, was assigned a Bernoulli(0.5) distribution to specify equal probability of inclusion (1) or exclusion (0) of the effect in the model (Durban et al. 2015).

A Poisson distribution ($N_{d(j),t} \sim Poisson(\lambda_{d(j),t})$) was used as a hierarchical prior for the distribution of abundances, and specified a model for the Poisson mean ($\lambda$) in terms of the number of whales passing each day ($d$), with an offset for the effort duration of each watch period, $E_{d(j),t}$ in decimal days (Laake et al., 2012):

$$ log(\lambda_{d(j),t}) = log(E_{d(j),t}) + model_{d,t}$$

$$ model_{d,t} = z_{d,t} f_{d,t} + (1 – z_{d,t}) g_{d,t} $$

where $z_{d,t}$ is an indicator function (see below). $f_{d,t}$ and $g_{d,t}$ also are explained below.

Days were specified as d = 0 to 90, where days were counted from 12:00am on 1 December, and we added an abundance of 0 whales passing for day 0 and 90 to anchor the fitted model when we assumed whales did not pass (Buckland et al. 1993).

Estimates were derived from either of two competing models ['Common' ($f$) and 'Specific' ($g$), e.g., @li2012] describing changes in abundance across each annual migration. The model contributing each daily estimate was indicated using stochastic binary indicator variables $z_{d,t}$, each assigned a Bernoulli(0.5) prior distribution. As such, the posterior distribution of each $z_{d,t}$ indicated the probability of a daily estimate conforming to the common trend, allowing flexibility for departures from this trend that may only exist on certain days in certain years to be identified and modeled (rather than assuming all counts from an entire year conform to or depart from a common trend). The total number of whales passing at the survey location during each migration was then estimated by summing the expected value from the model averaged number of whales passing each day ($d$) from 0 to 90 (Laake et al., 2012).

These estimates were then rescaled to account for the differential passage rate at night (Perryman et al., 1999), based on the nine hour day multiplicative correction factor [@rugh2005]. Specifically, we applied a constant night time correction factor that was assumed to be a Normally distributed fixed effect with mean of 1.0875 and standard deviation of 0.037 [@perryman1999a].

For the 'Common model' ($f_{d,t}$), we assumed a typical trend in abundance throughout each annual migration (e.g. Buckland et al., 1993), with abundance changes assumed Normally distributed around a migration mid-point. A Normal distribution was specified as a quadratic function of days, on the log scale:

$$f_{d,t} = a_t + b_t *d_t + c_t * d^2_t$$

where the mid-point of the migration curve for each year $t$ was derived by $–b_t/2a_t$. This assumed common migration curve allowed information to be 'borrowed' across years when needed, specifying association across years to strengthen inference about migration curves in years with relatively sparse counts. Each parameter was specified to be drawn from a hierarchical Normal distribution.

$$ a_t \sim Normal(\mu_a, \sigma_a)$$

$$ b_t \sim Normal(\mu_b, \sigma_b)$$

$$ c_t \sim Normal(\mu_c, \sigma_c)$$

and $\mu \sim Normal(0, 10)$ and $\sigma \sim Uniform(0, 10)$, for $a$, $b$, and $c$.

This hierarchical and random effects approach allowed the timing, level and extent of the Normal migration curve to vary annually around the general pattern, if supported by the data.

To acknowledge and incorporate deviations from the common Normal model, the selection of an alternative 'specific' migration model was allowed ($g_{d,t}$). The 'specific' model was a semi-parametric model that estimated the time trends independently for each year without making any prior assumptions about its form (e.g., Laake et al., 2012). In this model, the shape of the relationship of true abundance across days was determined by the data via penalized splines [@ruppert2002].

A linear (on the log scale) penalized spline was used to describe this relationship [@crainiceanu2005]:

$$ g_{d,t} = S_{0,t} + S_{1,t} * d_t + \sum_{k=1}^{m} \lambda_{k,t} (d_t – \kappa_{k,t}) $$ Where $S_{0,t}, S_{1,t}$ and $\lambda_{1,t}, \dots, \lambda_{m,t}$ were regression coefficients to be estimated separately for each year and $\kappa_{1,t} < \kappa_{2,t} < \dots < \kappa_{m,t}$ were fixed knots. To ensure the desired flexibility, we used m = 15 knots, which is a relatively large number. To avoid overfitting, the $\lambda$'s were penalized by assuming that they were Normally distributed random variables with mean 0 and standard deviation $\sim Uniform(0,10)$. The parameters $S_{0,t}, S_{1,t}$ were modeled as fixed effects with Normal(0, 10) prior distributions.

### Richards' function's characteristics {.unnumbered}

#### Effects of $S_1$ {.unnumbered}

The parameter $S_1 < 0$ defines how the curve decreases from its peak. The rate of decline slows down as $S_1$ becomes greater (i.e., closer to zero; Figure \@ref(fig:Figure-S1)). Because we make an assumption that there are no whales migrating after day 90 (or any other assumed "end" day), the lower bound of $S_1$ can be restricted to a certain value, (e.g., $-5 < S_1 < 0$).

```{r S1, echo=FALSE, message=FALSE}
S1 <- c(10, 5, 2.5, 1.2, 0.6, 0.3)
S2 <- 1.5
K <- 1
P <- 40
max.N <- 800

true.mean.N <- matrix(data = 0, nrow = 90, ncol = length(S1))

for (c in 1:length(S1)){
  for (d in 1:90){
    true.mean.N[d, c] <- floor(Richards_fcn(d = d, 
                                            S1 = -S1[c], 
                                            S2 = S2,
                                            K = K, 
                                            P = P, 
                                            min = 0, max = max.N)  )
    

  }
  
}

data.df <- data.frame(Day = rep(1:90, times = length(S1)),
                      mean.N = as.vector(true.mean.N),
               
                      S1 = rep(S1, each = 90))

p.S1 <- ggplot(data = data.df) +
  geom_path(aes(x = Day, y = mean.N), color = "red") +
  facet_wrap(~ S1) +
  ylab("")

save.fig.fcn(p.S1, 
             file.name = paste0("figures/S1_", dpi.set, "dpi.png"), 
             height = 5, width = 7,replace = T, dpi = dpi.set)

```

```{r Figure-S1, echo=FALSE, message=FALSE, fig.cap="Effects of $S_1$. In this example, $S_2 = 1.5$, $K = 1$, $P = 40$, $N_{max} = 800$."}

knitr::include_graphics(paste0("figures/S1_", dpi.set, "dpi.png"))

```

#### Effects of $S_2$ {.unnumbered}

The parameter $S_2 > 0$ defines how the curve increases to its peak. The rate of increase slows down as $S_2$ becomes larger (Figure \@ref(fig:Figure-S2)). Because we make an assumption that there are no whales migrating before day 1, the upper bound of $S_2$ can be restricted to a certain value (e.g., $0 < S_2 <5$).

```{r S2, echo=FALSE, message=FALSE}
S1 <- -1.5
S2 <- c(0.3, 0.6, 1.2, 2.5, 5, 10)
K <- 1
P <- 40
max.N <- 800

true.mean.N <- matrix(data = 0, nrow = 90, ncol = length(S2))

for (c in 1:length(S2)){
  for (d in 1:90){
    true.mean.N[d, c] <- floor(Richards_fcn(d = d, 
                                            S1 = S1, 
                                            S2 = S2[c],
                                            K = K, 
                                            P = P, 
                                            min = 0, max = max.N)  )
    
  }
  
}

data.df <- data.frame(Day = rep(1:90, times = length(S2)),
                      mean.N = as.vector(true.mean.N),

                      S2 = rep(S2, each = 90))

p.S2 <- ggplot(data = data.df) +
  geom_path(aes(x = Day, y = mean.N), color = "red") +
  facet_wrap(~ S2) +
  ylab("")

save.fig.fcn(p.S2, 
             file.name = paste0("figures/S2_", dpi.set, "dpi.png"), 
             height = 5, width = 7,replace = T, dpi = dpi.set)


```

```{r Figure-S2, echo=FALSE, message=FALSE, fig.cap="Effects of $S_2$. In this example, $S_1 = -1.5$, $K = 1$, $P = 40$, $N_{max} = 800$."}

knitr::include_graphics(paste0("figures/S2_", dpi.set, "dpi.png"))

```

#### Effects of K {.unnumbered}

The parameter $K > 0$ defines the flatness of the curve at its peak. Greater $K$ values correspond to flatter peaks (Figure \@ref(fig:Figure-K)). Similarly to $S_1$ and $S_2$, the upper bound of $K$ may be defined based on the assumption that the numbers of migrating gray whales are zero at day 1 and 90 (e.g., $0 < K < max(K)$). The upper bound ($max(K)$) may change when other parameters change, such as $S_1$ and $S_2$.

```{r K, echo=FALSE, message=FALSE}
S1 <- -2.5
S2 <- 2.5
K <- c(0.01, 0.1, 1, 2, 4, 8)
P <- 40
max.N <- 800

true.mean.N <- matrix(data = 0, nrow = 90, 
                           ncol = length(K))

for (c in 1:length(K)){
  for (d in 1:90){
    true.mean.N[d, c] <- floor(Richards_fcn(d = d, 
                                            S1 = S1, 
                                            S2 = S2,
                                            K = K[c], 
                                            P = P, 
                                            min = 0, max = max.N)  )
    
  }
  
}

data.df <- data.frame(Day = rep(1:90, times = length(K)),
                      mean.N = as.vector(true.mean.N),
                      K = rep(K, each = 90))

p.K <- ggplot(data = data.df) +
  geom_path(aes(x = Day, y = mean.N), color = "red") +
  facet_wrap(~ K) +
  ylab("")

save.fig.fcn(p.K, 
             file.name = paste0("figures/K_", dpi.set, "dpi.png"), 
             height = 5, width = 7,replace = T, dpi = dpi.set)



```

```{r Figure-K, echo=FALSE, message=FALSE, fig.cap="Effects of $K$. In this example, $S_1 = -2.5$, $S_2 = 2.5$, $P = 40$, $N_{max} = 800$."}

knitr::include_graphics(paste0("figures/K_", dpi.set, "dpi.png"))

```

#### Effects of P {.unnumbered}

The parameter $P$ defines the location of its peak (Figure \@ref(fig:Figure-P)).

```{r P, echo=FALSE, message=FALSE}
S1 <- -2.5
S2 <- 2.5
K <- 1.5
P <- c(20, 40, 60, 80)
max.N <- 800

true.mean.N <- matrix(data = 0, nrow = 90, 
                           ncol = length(P))

for (c in 1:length(P)){
  for (d in 1:90){
    true.mean.N[d, c] <- floor(Richards_fcn(d = d, 
                                            S1 = S1, 
                                            S2 = S2,
                                            K = K, 
                                            P = P[c], 
                                            min = 0, max = max.N)  )
    
  }
  
}

data.df <- data.frame(Day = rep(1:90, times = length(P)),
                      mean.N = as.vector(true.mean.N),
                      P = rep(P, each = 90))

p.P <- ggplot(data = data.df) +
  geom_path(aes(x = Day, y = mean.N), color = "red") +
  facet_wrap(~ P) +
  ylab("")

save.fig.fcn(p.P, 
             file.name = paste0("figures/P_", dpi.set, "dpi.png"), 
             replace = T, 
             height = 5, width = 7,dpi = dpi.set)


```

```{r Figure-P, echo=FALSE, message=FALSE, fig.cap="Effects of $P$. In this example, $S_1 = -2.5$, $S_2 = 2.5$, $K = 1.5$, $N_{max} = 800$."}

knitr::include_graphics(paste0("figures/P_", dpi.set, "dpi.png"))

```

### Trace plots of parameters with high rank-standardized R hat values {.unnumbered}

```{r Figure-trace-1, echo=FALSE, message=FALSE, fig.cap="Trace plots of parameters with high Rhat values" }

knitr::include_graphics(paste0("figures/M5_trace_1.png"))

```

```{r Figure-trace-2, echo=FALSE, message=FALSE, fig.cap="Trace plots of parameters with high Rhat values" }
knitr::include_graphics(paste0("figures/M5_trace_2.png"))

```

```{r Figure-trace-3, echo=FALSE, message=FALSE, fig.cap="Trace plots of parameters with high Rhat values" }
knitr::include_graphics(paste0("figures/M5_trace_3.png"))

```

#### Changes in S1 and S2 over time {.unnumbered}

```{r Figure-S1-posteriors, echo=FALSE, message=FALSE, fig.cap="Changes in the S1 parameter over time."}

knitr::include_graphics(paste0("figures/posteriors_S1_", dpi.set, "dpi.png"))

```

```{r Figure-S2-posteriors, echo=FALSE, message=FALSE, fig.cap="Changes in the S2 parameter over time."}

knitr::include_graphics(paste0("figures/posteriors_S2_", dpi.set, "dpi.png"))

```

### JAGS code for Model 5

model{

	# Define Richards function - this is the mean of "true" daily abundance
	# The realization is a Poisson deviate. 
	for (y in 1:n.year){
				
		for (t in 2:(n.days-1)){
			# year-specific K - results in conversion issues. 2022-11-09 
			# This was fixed by adding hyperparameters. 2024
			M1[t, y] <- (1 + (2 * exp(K) - 1) * exp((1/(-S1[y])) * (P[y] - t))) ^ (-1/exp(K))
			M2[t, y] <- (1 + (2 * exp(K) - 1) * exp((1/S2[y]) * (P[y] - t))) ^ (-1/exp(K))
			
			mean.N[t, y] <- (Max[y]) * (M1[t, y] * M2[t, y]) 
			
			# True N is station independent and partially observed
			# N = 0 when t = 1 or t = max.day
			N[t, y] ~ dpois(mean.N[t,y])
			
		}
	}  

	# observations are from the realized abundance: N-mixture part with 
	# binomial sampling
	
	# force n = 0 for t = 1 and t = max.day
	for (y in 1:n.year){
		for (s in 1:n.station[y]){
			n[1, s, y] ~ dbin(0, 0)
			n[(periods[y,s]), s, y] ~ dbin(0,0)
		}
	}
		
	for (y in 1:n.year){
		for (s in 1:n.station[y]){
			for (d in 2:(periods[y,s]-1)){
				# observation per period d								
				n[d, s, y] ~ dbin(obs.prob[d,s,y], N[day[d,s,y], y])
				log.lkhd[(d-1),s,y] <- logdensity.bin(n[d,s,y], obs.prob[d, s, y], N[day[d,s,y], y])
				
				# probability is mean + covariate effects and watch.length as an offset
				logit(obs.prob[d,s,y]) <- mean.prob +
								(BF.Fixed * bf[(d-1),s,y]) +
								(VS.Fixed * vs[(d-1),s,y]) +
								(OBS.RF[obs[d,s,y]]) +
								log(watch.length[(d-1), s, y])

			}
		}
	}
		
	# It doesn't make sense to make it [0,1]...
	mean.prob ~ dunif(-1.5, 3) 
	
	# priors for Richards function parameters. 
	for (y in 1:n.year){
		Max[y] ~ dnorm(2000, 1/(800^2))T(0,)
		S1[y] ~ dgamma(S1.alpha, S1.beta)  
		S2[y] ~ dgamma(S2.alpha, S2.beta)  
		P[y] ~ dunif(30, 60) #dgamma(P.alpha, P.beta) 	
	}

	# Uniform distributions were adjusted to capture entire posterior 
	# distributions of the hyper-parameters. 
	
	S1.alpha ~ dunif(0.1, 50)
	S1.beta ~ dunif(0.01, 5)
	
	S2.alpha ~ dunif(0.1, 50)
	S2.beta ~ dunif(0.01, 10)
	
	#P.alpha ~ dunif(1, 10000)
	#P.beta ~ dunif(0.01, 250)
	
	K ~ dgamma(K.alpha, K.beta) 
	
	# K.beta <= K.alpha seems to work better with the gamma distribution for K
	# So, I add a random number to K.beta to create K.alpha
	K.beta ~ dunif(0, 5000)
	K.unif ~ dunif(0, 500)
	K.alpha <- K.beta + K.unif
	     
	## Observer random effects
	for(o in 1:n.obs){
		OBS.RF[o] ~ dnorm(0,tau.Obs)
	}#o
  
	sigma.Obs ~ dunif(0, 3) #dgamma(1, 0.1)
	tau.Obs <- pow(sigma.Obs,-2)
     
	## Beaufort and visibility	
	BF.Fixed ~ dnorm(0,0.001) 
	VS.Fixed ~ dnorm(0,0.001) 
  
	### Summaries, Abundance Estimates, and Other Derived Quantities 
	for(t in 1:n.year){
		#N is Poisson deviates so they should be integers
		#raw.unrounded[t] <- sum(N[1:n.days,t])
		#Raw.Est[t] <- round(raw.unrounded[t])
		
		Raw.Est[t] <- sum(N[1:n.days, t])
		# multiply raw estimates by correction factor for nighttime passage rates (below)
		Corrected.Est[t] <- Raw.Est[t] * corr.factor 
	}#t
  
	# Correction factor for nighttime passage rates (Perryman et al. 1999 Marine Mammal Science):
	corr.factor ~ dnorm(mean.corr, tau.corr)
	mean.corr <- 1.0875
	sd.corr <- 0.03625
	tau.corr <- pow(sd.corr,-2)
  		
	 
  # In the paper, Perryman et al. stated that "... the multiplicative correction 
  # factor for the entire visual survey based on a 15-h nocturnal (i.e., non-survey)
  # period would be 1 + 0.175f (SE = 0.116 * (14/24)), where f is the fraction of total
  # whales migrating afer 15 January." So, fixing the mean to be 1.0875, it assumes that
  # the median date of migration is Jan 15 (1 + 0.175 * 0.5 = 1.0875). This may have to
  # be fixed to adjust for the changing median migration date. SE of 0.03625 was not found
  # in the paper... 0.116*(14/24) = 0.0677	
  
  # There was a typographic error in Perryman et al. Laake et al. (2012) stated that "... a
  # standard error of f x 15/24 x 0.116 after correcting the typographic errors in Perryman
  # et al. (1999).
}